{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#! switch_R 3.3\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabinasloman/git/talking-politics/pyspan/config.py:19: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 499, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/tornado/ioloop.py\", line 1073, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 456, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 486, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 438, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2724, in run_cell\n",
      "    self.events.trigger('post_run_cell')\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/IPython/core/events.py\", line 74, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/ipykernel/pylab/backend_inline.py\", line 164, in configure_once\n",
      "    activate_matplotlib(backend)\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/IPython/core/pylabtools.py\", line 315, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/matplotlib/pyplot.py\", line 231, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/matplotlib/__init__.py\", line 1425, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/matplotlib/backends/__init__.py\", line 17, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  mpl.use(settings[\"mpl_backend\"])\n",
      "/Users/sabinasloman/git/talking-politics/pyspan/Purger.py:203: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 203 of the file /Users/sabinasloman/git/talking-politics/pyspan/Purger.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup=BeautifulSoup(text)\n",
      "/Users/sabinasloman/git/talking-politics/pyspan/Purger.py:215: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 215 of the file /Users/sabinasloman/git/talking-politics/pyspan/Purger.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup=BeautifulSoup(text)\n",
      "/Users/sabinasloman/git/talking-politics/pyspan/ratings_task/data.py:27: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  all_ = pd.concat((partisan, antonyms))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import statsmodels.stats.api as sms\n",
    "from pyspan import cross_validation as cv\n",
    "from pyspan.ratings_task.analysis import *\n",
    "from pyspan.config import *\n",
    "from pyspan.valence import *\n",
    "assert settings[\"mode\"] == \"crec\"\n",
    "INPUT_DIR = paths[\"input_dir\"]\n",
    "METRICS_DIR = paths[\"metrics_dir\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n",
      "37.65986394557823 0.8431946541195345\n",
      "74 72\n",
      "0.8163265306122449\n"
     ]
    }
   ],
   "source": [
    "# n\n",
    "print len(minidf)\n",
    "# Mean age and standard error of the mean\n",
    "print np.mean(minidf.age[~np.isnan(minidf.age)]), stats.sem(minidf.age[~np.isnan(minidf.age)])\n",
    "# n identifying as male/female\n",
    "print len(minidf.loc[minidf.gender == \"M\"]), len(minidf.loc[minidf.gender == \"F\"])\n",
    "# % voted\n",
    "print np.mean(minidf.voted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = minidf[map(str, p_ixs)].values\n",
    "ddat = dat[:,partisan.loc[p_ixs].PKL_D > 0]\n",
    "rdat = dat[:,partisan.loc[p_ixs].PKL_D < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmu = np.mean(rdat[~np.isnan(rdat)])\n",
    "dmu = np.mean(ddat[~np.isnan(ddat)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean participant rating on the Republican and Democratic items (add 1 to obtain the numbers reported in the paper, as those are described on a 1--6 scale, while these are on a 0--5 scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.8064862927790717, 2.2983811626195734)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmu, dmu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform one- and two-sample $t$-tests, using the clustered standard errors function provided by Arai (2011)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "source(\"../clmclx.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-sample test (judgments of the Republican words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: Loading required package: zoo\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: \n",
      "Attaching package: ‘zoo’\n",
      "\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    as.Date, as.Date.numeric\n",
      "\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1] 5580\n",
       "[1] \"0.0812042442925946 3.77426445438914 8.10728057510572e-05\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i rdat\n",
    "y <- c(t(rdat)) - 2.5\n",
    "nr <- dim(rdat)[1]\n",
    "kr <- dim(rdat)[2]\n",
    "idxn <- rep(1:nr, each = kr)\n",
    "idxn <- idxn[which(!is.na(y))]\n",
    "idxk <- rep(1:kr, nr)\n",
    "idxk <- idxk[which(!is.na(y))]\n",
    "fit <- lm(y ~ 1)\n",
    "print(fit$df.residual)\n",
    "fit <- lm(y ~ 1)\n",
    "res <- mclx(fit, 1, idxn, idxk)\n",
    "se <- res[1,2]\n",
    "t <- res[1,3]\n",
    "p <- res[1,4] / 2\n",
    "print(paste(se, t, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-sample test (judgments of the Democratic words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1] 5435\n",
       "[1] \"0.121547860301295 -1.65876089369777 0.048610842215481\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i ddat\n",
    "y <- c(t(ddat)) - 2.5\n",
    "nd <- dim(ddat)[1]\n",
    "kd <- dim(ddat)[2]\n",
    "idxn <- rep(1:nd, each = kd)\n",
    "idxn <- idxn[which(!is.na(y))]\n",
    "idxk <- rep(1:kd, nd)\n",
    "idxk <- idxk[which(!is.na(y))]\n",
    "fit <- lm(y ~ 1)\n",
    "print(fit$df.residual)\n",
    "res <- mclx(fit, 1, idxn, idxk)\n",
    "se <- res[1,2]\n",
    "t <- res[1,3]\n",
    "p <- res[1,4] / 2\n",
    "print(paste(se, t, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two-sample test (testing the difference in means)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1] 11015\n",
       "[1] \"0.14523446030682 3.49851632378421 0.00023485058296235\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "y <- c(t(ddat), t(rdat))\n",
    "idvs <- c(rep(1:nd, each = kd), rep((nd+1):(nd+nr), each = kr))\n",
    "idxn <- idvs[which(!is.na(y))]\n",
    "items <- c(rep(1:kd, nd), rep((kd+1):(kd+kr), nr))\n",
    "idxk <- items[which(!is.na(y))]\n",
    "groups <- c(rep(0, nd*kd), rep(1, nr*kr))\n",
    "fit <- lm(y ~ groups)\n",
    "print(fit$df.residual)\n",
    "res <- mclx(fit, 1, idxn, idxk)\n",
    "se <- res[2,2]\n",
    "t <- res[2,3]\n",
    "p <- res[2,4] / 2\n",
    "print(paste(se, t, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect size\n",
    "\n",
    "Using the formula from [Rouder et al. (2012)](http://pcl.missouri.edu/sites/default/files/Rouder.JMP_.2012.pdf) and explained in [Jake Westfall's blog post](http://jakewestfall.org/blog/index.php/2016/03/25/five-different-cohens-d-statistics-for-within-subject-designs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: Loading required package: Matrix\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1] 0.4063363\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R \n",
    "library(\"lme4\")\n",
    "mod <- lmer(y ~ groups + (groups|idvs) + (1|items))\n",
    "delta <- summary(mod)$coefficients[2,1]\n",
    "sig <- sigma(mod)\n",
    "delta / sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-level analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(i):\n",
    "    polarity = signal_df.loc[partisan.loc[i][\"word\"]].rmetric > 0\n",
    "    judgments = minidf[str(i)]\n",
    "    judgments = judgments[~np.isnan(judgments)]\n",
    "    return len(judgments[judgments > 2.5 if polarity else judgments < 2.5]), len(judgments)\n",
    "v_calc_accuracy = np.vectorize(calc_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, n = v_calc_accuracy(p_ixs)\n",
    "p = p / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of words correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dixs = [ i for i in range(len(p_ixs)) if signal_df.loc[partisan.loc[p_ixs[i]][\"word\"]].dmetric > 0 ]\n",
    "rixs = [ i for i in range(len(p_ixs)) if signal_df.loc[partisan.loc[p_ixs[i]][\"word\"]].rmetric > 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 26, 50)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p[dixs] > .5).sum(), (p[rixs] > .5).sum(), (p > .5).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average item-level accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5861659366943126, 0.023279929676539295)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(p), stats.sem(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.701297121234333, 0.00020504711762496346, 74)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t, p_ = stats.ttest_1samp(p, .5)\n",
    "t, p_ / 2, len(p)-1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mann-Whitney U test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word, compute the mean of the judgments across subjects, and perform a Mann-Whitney U test to determine whether the Republican words are as likely to have a higher rank than Democratic words as to have a lower rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmeans = np.mean(ddat, axis=0)\n",
    "rmeans = np.mean(rdat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=416.0, pvalue=0.001197878011080101)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.mannwhitneyu(dmeans, rmeans, alternative=\"less\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure\n",
    "\n",
    "This creates a csv file called `figure.csv` with columns `word`, `dmetric`, `rmetric`, `ratings_mean`, `ratings_std` and `frac_r`. The script `figures/figures_2_4.py` creates Fig 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceps = PerceptualData(ixs = p_ixs)\n",
    "perceps.get_discriminability_by_word()\n",
    "perceps.item_level_res(signal_df, savetofile=\"figure.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participant-level analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in less\n",
      "  \n",
      "/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in greater\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "signals = np.array(signals)\n",
    "prop_right = []\n",
    "for i in minidf.index:\n",
    "    perceps = minidf.loc[i][map(str, p_ixs)].values\n",
    "    perceps = np.array(list(map(float, perceps)))\n",
    "    nright = len(perceps[((perceps < 2.5) & (signals < 0)) | ((perceps > 2.5) & (signals > 0))])\n",
    "    prop_right.append(nright / len(perceps[~np.isnan(perceps)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5861519836862302, 0.005877149218751656)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(prop_right), stats.sem(prop_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14.658804886449602, 8.686107635321322e-31, 146, 127)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t, p = stats.ttest_1samp(prop_right, .5)\n",
    "t, p / 2, len(prop_right) - 1, \\\n",
    "    len([ prop for prop in prop_right if prop > .5 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracies on the Democratic items..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in less\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5751776557899008, 0.010990721007601595)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_right = []\n",
    "for i in minidf.index:\n",
    "    perceps = minidf.loc[i][map(str, p_ixs)].values\n",
    "    perceps = np.array(list(map(float, perceps)))\n",
    "    perceps = perceps[signals < 0]\n",
    "    nright = len(perceps[perceps < 2.5])\n",
    "    prop_right.append(nright / len(perceps[~np.isnan(perceps)]))\n",
    "np.mean(prop_right), stats.sem(prop_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and on the Republican items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in greater\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5968637810743074, 0.009098182686866388)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_right = []\n",
    "for i in minidf.index:\n",
    "    perceps = minidf.loc[i][map(str, p_ixs)].values\n",
    "    perceps = np.array(list(map(float, perceps)))\n",
    "    perceps = perceps[signals > 0]\n",
    "    nright = len(perceps[perceps > 2.5])\n",
    "    prop_right.append(nright / len(perceps[~np.isnan(perceps)]))\n",
    "np.mean(prop_right), stats.sem(prop_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = np.array(signals)\n",
    "prop_right = []\n",
    "for i in minidf.index:\n",
    "    perceps = minidf.loc[i][map(str, p_ixs)].values\n",
    "    perceps = np.array(list(map(float, perceps)))\n",
    "    rjudgments = perceps[signals > 0]\n",
    "    rjudgments = rjudgments[~np.isnan(rjudgments)]\n",
    "    djudgments = perceps[signals < 0]\n",
    "    djudgments = djudgments[~np.isnan(djudgments)]\n",
    "    prop_right.append(cohensd(rjudgments, djudgments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4033623843300961, 0.02592686178162114)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(prop_right), stats.sem(prop_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.557701804698516, 4.249474340581165e-33, 146)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t, p = stats.ttest_1samp(prop_right, 0)\n",
    "t, p / 2, len(prop_right) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for individual-level differences in discriminability between Democrats and Republicans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_right_r = []\n",
    "for i in minidf.loc[minidf.party == \"Republican\"].index:\n",
    "    perceps = minidf.loc[i][map(str, p_ixs)].values\n",
    "    perceps = np.array(list(map(float, perceps)))\n",
    "    rjudgments = perceps[signals > 0]\n",
    "    rjudgments = rjudgments[~np.isnan(rjudgments)]\n",
    "    djudgments = perceps[signals < 0]\n",
    "    djudgments = djudgments[~np.isnan(djudgments)]\n",
    "    prop_right_r.append(cohensd(rjudgments, djudgments))\n",
    "    \n",
    "prop_right_d = []\n",
    "for i in minidf.loc[minidf.party == \"Democrat\"].index:\n",
    "    perceps = minidf.loc[i][map(str, p_ixs)].values\n",
    "    perceps = np.array(list(map(float, perceps)))\n",
    "    rjudgments = perceps[signals > 0]\n",
    "    rjudgments = rjudgments[~np.isnan(rjudgments)]\n",
    "    djudgments = perceps[signals < 0]\n",
    "    djudgments = djudgments[~np.isnan(djudgments)]\n",
    "    prop_right_d.append(cohensd(rjudgments, djudgments))\n",
    "    \n",
    "prop_right_r = np.array(prop_right_r)\n",
    "prop_right_d = np.array(prop_right_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24318006794665611,\n",
       " 0.05637666271252923,\n",
       " 0.4697651543599723,\n",
       " 0.035351293795834404)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(prop_right_r), stats.sem(prop_right_r), \\\n",
    "    np.mean(prop_right_d), stats.sem(prop_right_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.4050653585985096, 0.0011332554083023421, 65.56602803605274)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = sms.CompareMeans(sms.DescrStatsW(prop_right_d), sms.DescrStatsW(prop_right_r))\n",
    "cm.ttest_ind(usevar=\"unequal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression further investigating individual differences\n",
    "\n",
    "Correct ~ logodds(word) + log(P(word)) + party(participant) + valence(word) + party(participant) $\\times$ valence(word) $\\times$ is\\_republican(word) + party\\_identity + party\\_identity $\\times$ party(participant) + political\\_engagement + political\\_engagement $\\times$ party(participant) + $\\ldots{}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in greater\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "lr_data = minidf.copy()\n",
    "lr_data = lr_data.loc[lr_data.party.isin([ \"Democrat\", \"Republican\" ])]\n",
    "n = len(lr_data)\n",
    "items = partisan.loc[p_ixs].word\n",
    "n_vars = 9\n",
    "\n",
    "# Add dummy columns\n",
    "for i in range(1, n):\n",
    "    ids = np.zeros(n)\n",
    "    ids[i] = 1\n",
    "    lr_data[\"participant{}\".format(i)] = ids\n",
    "    \n",
    "Y_full = np.ravel(lr_data[map(str, p_ixs)])\n",
    "Y = Y_full > 2.5\n",
    "X = np.full((n * len(p_ixs), n_vars + n - 1 + len(p_ixs) - 1), np.nan)\n",
    "# The log odds that the word was said by a Republican\n",
    "vf = np.vectorize(lambda word: abs(signal_df.loc[word][\"rmetric\"]))\n",
    "X[:,0] = np.tile(vf(items), n)\n",
    "# The log probability of hearing the word\n",
    "vf = np.vectorize(lambda word: math.log(sum(freq_df.loc[(word,[\"dmetric\",\"rmetric\"])].values) / n_utterances, 2))\n",
    "X[:,1] = np.tile(vf(items), n)\n",
    "# Participant's political identity\n",
    "vf = np.vectorize(lambda pid: 1 if pid == \"Republican\" else -1 if pid == \"Democrat\" else 0)\n",
    "pids = np.repeat(vf(lr_data.party), len(p_ixs))\n",
    "X[:,2] = pids\n",
    "# Valence of word\n",
    "vf = np.vectorize(lambda word: get_valence(word)[0]-5)\n",
    "X[:,3] = np.tile(vf(items), n)\n",
    "# Valence of word * Participant's party identity * Word is Republican\n",
    "vf = np.vectorize(lambda word: signal_df.loc[word][\"rmetric\"])\n",
    "v = np.tile(vf(items), n)\n",
    "X[:,4] = pids * X[:,3] * np.sign(v) \n",
    "# Party identity\n",
    "X[:,5] = np.repeat(lr_data.party_identity, len(p_ixs))\n",
    "# Party affiliation x party identity\n",
    "X[:,6] = pids * X[:,5]\n",
    "# Political engagement\n",
    "X[:,7] = np.repeat(lr_data.political_engagement, len(p_ixs))\n",
    "# Party affiliation x political engagement\n",
    "X[:,8] = pids * X[:,7]\n",
    "\n",
    "# Participant dummies\n",
    "for i in range(n_vars, n_vars + n - 1):\n",
    "    X[:,i] = np.repeat(lr_data[\"participant{}\".format(i-n_vars+1)], len(p_ixs))\n",
    "\n",
    "# Item dummies\n",
    "for i in range(1,len(p_ixs)):\n",
    "    ix = n_vars+n-1+i-1\n",
    "    ind_vec = np.zeros(len(p_ixs))\n",
    "    ind_vec[i] = 1\n",
    "    X[:,ix] = np.tile(ind_vec, n)\n",
    "\n",
    "vf = np.vectorize(lambda word: signal_df.loc[word][\"rmetric\"] > 0)\n",
    "polarity = vf(items)\n",
    "polarity = np.tile(polarity, n)\n",
    "Y = (Y & polarity) | (~Y & ~polarity)\n",
    "\n",
    "Y = Y[~np.isnan(Y_full)]\n",
    "X = X[~np.isnan(Y_full)]\n",
    "Y_full = Y_full[~np.isnan(Y_full)]\n",
    "X = X[~np.isnan(Y)]\n",
    "Y_full = Y_full[~np.isnan(Y)]\n",
    "Y = Y[~np.isnan(Y)]\n",
    "Y = Y[~np.isnan(X).any(axis = 1)]\n",
    "Y_full = Y_full[~np.isnan(X).any(axis = 1)]\n",
    "X = X[~np.isnan(X).any(axis = 1),:]\n",
    "\n",
    "# Since we don't have valence data for some of the words, their observations will be deleted and their indicator columns will\n",
    "# just be clutter (and cause problems with model estimation)\n",
    "colixs = np.arange(0,X.shape[1])\n",
    "to_del = np.where(np.apply_along_axis(lambda x: np.all(x == 0), 0, X[:,n_vars+n-1:-1]))[0] + n_vars+n-1\n",
    "colixs = [ colix for colix in colixs if colix not in to_del ]\n",
    "X = X[:,colixs]\n",
    "\n",
    "X = stats.mstats.zscore(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6726, 174), 166)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, np.linalg.matrix_rank(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6726, 9), 9)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,:n_vars].shape, np.linalg.matrix_rank(X[:,:n_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix with fixed effects is not full rank, but the matrix without fixed effects is, so estimate the model without fixed effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabinasloman/.pyenv/versions/2.7.17/envs/politics-test-bed/lib/python2.7/site-packages/sklearn/model_selection/_split.py:2052: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.32761308,  0.13406804, -0.08107084,  0.19290955,  0.30146516,\n",
       "        0.02403751, -0.01787496,  0.06184514,  0.00804689])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = SparseLR(Y, X[:,:n_vars]); logit.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6726, 0.6130553253028238)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.n, logit.auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many items are effectively excluded on the basis of missing valence data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vf = np.vectorize(lambda word: get_valence(word)[0]-5)\n",
    "v = vf(items)\n",
    "len(v[np.isnan(v)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
